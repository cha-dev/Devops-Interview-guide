1. What is K8s, and why do we use it?

Kubernetes (K8s, pronounced "K8s") is an open-source container orchestration platform originally developed by Google.
It helps you manage, scale, and automate applications that run inside containers (like Docker).

Think of it as a control system for your containerized apps — ensuring they run reliably in production.

🔑 Why Do We Use Kubernetes?

Because running just a few containers with Docker is easy…
But in production, with hundreds or thousands of containers, things get complex.
Kubernetes solves these challenges:

Automated Deployment & Scaling
Automatically starts containers and scales them up/down depending on demand.
Example: If traffic spikes, K8s adds more pods (containers).
High Availability & Self-Healing
If a container crashes, Kubernetes restarts it automatically.
Ensures the app keeps running with minimal downtime.
Load Balancing & Service Discovery
Distributes traffic across containers so no single one is overloaded.
Provides stable DNS/IP for apps (even if containers are constantly changing).
Portability (Multi-Cloud / Hybrid)
Run apps on AWS, Azure, GCP, or on-premise without big changes.
Efficient Resource Utilization
Schedules containers across nodes to make the best use of CPU/memory.
Rolling Updates & Rollbacks
Deploy new versions of apps without downtime.
Rollback easily if something breaks.
Docker → puts your app in a container.
Kubernetes (K8s) → manages those containers at scale.

2. Explain the diff between pods, deployments and Replicasets?

1️⃣ Pod

The smallest deployable unit in Kubernetes.
A pod wraps one or more containers (usually one).
Containers in a pod share:
Network (same IP/port space)
Storage volumes
Pods are ephemeral → if a pod dies, it’s gone.

👉 Example: A single pod running nginx:latest container.

2️⃣ ReplicaSet

Ensures a specific number of pod replicas are running at all times.
If a pod crashes or node fails → ReplicaSet spins up a new one.
If you scale from 2 replicas → 5 replicas, ReplicaSet ensures exactly 5 pods exist.

👉 Example: ReplicaSet with 3 replicas → always keeps 3 nginx pods running.

3️⃣ Deployment

The highest-level abstraction among the three.
Manages ReplicaSets for you.
Provides advanced features like:
Rolling updates (zero-downtime upgrades of your app).
Rollbacks (go back to a previous stable version if something fails).
Scaling up/down easily.

👉 Example: A Deployment defines “I want 3 replicas of nginx running with image v1.14.2” → it creates a ReplicaSet → which keeps 3 pods alive.

📊 Hierarchy (in simple terms)
Deployment → manages ReplicaSets → manage Pods → run Containers

⚖️ Quick Analogy

Pod = a single coffee cup ☕ (contains coffee = container).

ReplicaSet = coffee machine that makes sure you always have 3 full cups ready.

Deployment = café manager who decides recipes, upgrades to better beans, and ensures the machine always works smoothly.

3. What is a Service in K8s?

A Service is an abstraction that defines a stable network endpoint (IP + DNS name) for accessing a set of Pods.
Since Pods are ephemeral (they get replaced, new IPs assigned), a Service ensures:
Clients don’t need to track pod IPs.
Traffic is automatically load-balanced across healthy pods.

🔑 How It Works

Pods have dynamic IPs
Each pod gets its own IP, but it changes if the pod dies/restarts.
Service groups pods using labels
You define a selector (e.g., app=nginx).
The Service automatically finds all pods with that label.
Kube-proxy sets up routing
Runs on every node, updates iptables/IPVS rules.
Ensures traffic to the Service’s IP/Port is forwarded to one of the matching pods.
Stable ClusterIP / DNS name
Service gets a fixed IP (ClusterIP) and a DNS name (via kube-dns/CoreDNS).
Other pods can always reach it at http://<service-name>.<namespace>.svc.cluster.local.

⚙️ Types of Services

ClusterIP (default)
Internal-only access inside the cluster.
Example: A frontend pod talking to a backend pod.

NodePort
Exposes service on a static port across all nodes (<NodeIP>:<NodePort>).
Useful for simple external access.
LoadBalancer
Works with cloud providers (AWS, GCP, Azure).
Provisions an external load balancer that routes to the service.
ExternalName
Maps service to an external DNS name (like a database outside the cluster).

📊 Visual Flow
Client → Service (stable IP/DNS) → kube-proxy → Pod(s)

📌 Example YAML (ClusterIP Service for nginx)
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80         # Service port
      targetPort: 80   # Pod container port
  type: ClusterIP


👉 Any request to nginx-service:80 will be routed to one of the pods with app=nginx.

✅ In short:
A K8s Service = stable entry point + load balancer for a group of pods, ensuring apps can communicate reliably inside (and outside) the cluster.

Networking and Storage
------------------------
4. Service vs Ingress
1️⃣ Service

Purpose: Provides a stable network endpoint (IP/DNS) and load balances traffic to pods.

Works at the TCP/UDP layer (Layer 4 of OSI).

Exposes your app inside the cluster (ClusterIP) or outside (NodePort/LoadBalancer).

Example: A Service might expose your nginx pod at http://<cluster-ip>:80.

2️⃣ Ingress

Purpose: Manages HTTP/HTTPS traffic routing into your cluster.

Works at the application layer (Layer 7) → can inspect URLs/hosts.

Needs an Ingress Controller (like NGINX, Traefik, HAProxy).

Example:

http://myapp.com/api → routes to backend service A.

http://myapp.com/web → routes to backend service B.

⚖️ Analogy

Service = Think of it like the street address of a building. It ensures you can reach the building.

Ingress = Like the reception desk inside the building that decides which department (API, UI, payments) you should go to.

📊 Visual
Client → Ingress (rules/hostnames/paths) → Service → Pods

📌 Example
Service (backend API)
apiVersion: v1
kind: Service
metadata:
  name: api-service
spec:
  selector:
    app: api
  ports:
    - port: 80
      targetPort: 8080
  type: ClusterIP

Ingress (routing rules)
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
    - host: myapp.com
      http:
        paths:
          - path: /api
            pathType: Prefix
            backend:
              service:
                name: api-service
                port:
                  number: 80


👉 Here, traffic to http://myapp.com/api goes to api-service, which forwards to backend pods.

✅ In short:

Service = exposes and load balances pods.

Ingress = provides smart HTTP/HTTPS routing into services.

5. What are the different types of services in K8S?

Types of Services in Kubernetes
1️⃣ ClusterIP (default)

Exposes the service inside the cluster only (not accessible from outside).

Allocates a stable internal cluster IP.

Good for communication between microservices inside the cluster.

👉 Example:
Frontend app (Pod) → calls ClusterIP service → Backend app (Pod).

2️⃣ NodePort

Exposes the service externally on a fixed port of each node (range: 30000–32767).

Traffic:

Client → NodeIP:NodePort → Service → Pods


Easy to test, but not great for production (not scalable, fixed port range).

3️⃣ LoadBalancer

Cloud-provider dependent (works in AWS, Azure, GCP, etc.).

Provisions an external load balancer from the cloud provider.

Routes external traffic → LB → NodePort/ClusterIP → Pods.

Most common way to expose apps to the internet in production.

4️⃣ ExternalName

Maps a Service to an external DNS name (instead of pods).

Doesn’t create a real proxy or ClusterIP, just returns a CNAME record.

Example:

externalName: database.mycompany.com

5️⃣ Headless Service (ClusterIP: None)

No cluster IP allocated.

Instead of load-balancing, it returns pod IPs directly.

Useful for stateful apps (like databases, Kafka, etc.) where each pod needs a unique identity.

📊 Quick Summary
Service Type	Scope / Usage
ClusterIP	Default, internal communication only.
NodePort	Exposes app on each node’s IP & port.
LoadBalancer	External access via cloud load balancer.
ExternalName	Maps service to an external DNS name.
Headless	Direct pod discovery, no load balancing.

✅ In interviews, you can phrase it like:
Kubernetes supports ClusterIP (internal only), NodePort (fixed node port), LoadBalancer (cloud LB), ExternalName (maps DNS), and Headless services (direct pod discovery). In production, LoadBalancer + Ingress are commonly used for external access.

6. How does Ingress work in Kubernetes?

An Ingress in Kubernetes is an API object that manages external HTTP/HTTPS traffic into your cluster. It acts like a smart router that sits at the edge of your cluster and forwards traffic to the right Service → Pods.

🛠️ How It Works

Ingress Controller
Ingress by itself is just a set of rules.
To make it work, you need an Ingress Controller (like NGINX Ingress Controller, Traefik, or cloud-managed ones like GKE Ingress, AWS ALB Ingress).
The controller listens for Ingress objects and configures the load balancer/reverse proxy.
Ingress Resource (Rules)
You define routing rules (hostnames & paths).

Example:

myapp.com/api → Service A

myapp.com/web → Service B

Traffic Flow

Client → Ingress Controller (NGINX, ALB, etc.) → Service → Pods

📌 Example Ingress YAML
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    - host: myapp.com
      http:
        paths:
          - path: /api
            pathType: Prefix
            backend:
              service:
                name: api-service
                port:
                  number: 80
          - path: /web
            pathType: Prefix
            backend:
              service:
                name: web-service
                port:
                  number: 80

🔑 Key Features of Ingress

Host-based routing → api.myapp.com vs web.myapp.com.

Path-based routing → /api vs /web.

TLS/SSL termination → Offloads HTTPS at the Ingress.

Load balancing → Distributes traffic across pods via Services.

Rewrite & redirects → Change incoming paths before forwarding.

⚖️ Analogy

Service = The street address of a building.

Ingress = The reception desk that checks your request (host/path) and sends you to the right department.

✅ In short:
Ingress works with an Ingress Controller to provide smart Layer 7 routing, SSL termination, and domain/path-based traffic management to services inside the cluster.

Would you like me to also create a diagram showing how:
Client → Ingress → Service → Pods works visually?

7. PV (PersistentVolume) and PVC (PersistentVolumeClaim) are storage concepts in Kubernetes. They solve the problem of pods losing data when they get deleted, rescheduled, or restarted.

📦 PersistentVolume (PV)

A piece of storage in the cluster.

Can be provided by:

Local disk

NFS (Network File System)

Cloud storage (EBS in AWS, Persistent Disk in GCP, Azure Disk, etc.)

Managed by the cluster admin.

Exists independently of pods (not tied to lifecycle of a pod).

Think of it as: “I have a storage drive available in the cluster.”

📜 PersistentVolumeClaim (PVC)

A request for storage by a developer/application.

PVCs specify:

Size (e.g., 10Gi)

Access mode (e.g., ReadWriteOnce, ReadOnlyMany, ReadWriteMany)

Kubernetes then matches the PVC with an available PV.

Think of it as: “I need a 10GB storage drive with read/write access.”

🛠️ How They Work Together

Admin creates a PV (10Gi storage from AWS EBS).

Developer creates a PVC asking for 5Gi storage.

Kubernetes binds PVC to a PV that satisfies the request.

The pod uses the PVC to mount the storage.

📌 Example YAML
PersistentVolume (PV)
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-volume
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /mnt/data

PersistentVolumeClaim (PVC)
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-claim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

Pod using PVC
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: app
      image: nginx
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: my-storage
  volumes:
    - name: my-storage
      persistentVolumeClaim:
        claimName: pvc-claim

✅ Quick Summary

PV = Actual storage (supply).

PVC = Request for storage (demand).

Pod mounts PVC → PVC binds to PV → Pod gets persistent storage.

8. Let’s extend this to StorageClass and Dynamic Provisioning, which is what most real-world Kubernetes clusters (AWS, GCP, Azure) actually use.

📂 StorageClass in Kubernetes

A StorageClass defines how storage should be provisioned dynamically.

Instead of an admin manually creating PVs, Kubernetes can automatically create them on-demand using the StorageClass.

Think of it as:

PV = The actual disk.

PVC = The request for storage.

StorageClass = The blueprint or template for how disks should be created.

🔑 Why StorageClass?

Automates PV creation.

Lets you define storage type, performance tier, and replication policy.

Cloud providers support multiple storage types:

AWS → gp2/gp3 (SSD), io1 (IOPS SSD), sc1 (HDD)

GCP → standard, balanced, SSD persistent disk

Azure → Standard_LRS, Premium_LRS, etc.

📌 Example: StorageClass (AWS EBS)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-storage
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  fsType: ext4
reclaimPolicy: Delete


provisioner: Defines which plugin/cloud driver to use.

parameters: Defines storage type (e.g., gp3 SSD in AWS).

reclaimPolicy: What happens when PVC is deleted (Delete or Retain).

📌 PVC with StorageClass
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: app-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast-storage
  resources:
    requests:
      storage: 20Gi


👉 When this PVC is created, Kubernetes:

Looks at the storageClassName (fast-storage).

Dynamically provisions a new PV (EBS volume in AWS).

Binds the PVC to that PV automatically.

🔄 Flow: Dynamic Provisioning

Pod requests PVC.

PVC references a StorageClass.

StorageClass provisions PV dynamically (from AWS/GCP/Azure).

Pod mounts the PVC → gets persistent storage.

✅ Quick Summary

PV = Actual disk.

PVC = Request for disk.

StorageClass = Template for automatic PV creation (dynamic provisioning).

In production, almost nobody creates PVs manually → StorageClass + PVC is the standard.

Scaling and scheduling 

9. How does HPA work?

HPA = Horizontal Pod Autoscaler

It automatically adjusts the number of pods in a deployment, replica set, or stateful set.
Scaling happens based on CPU, memory, or custom metrics.
Helps keep apps performant while saving resources/costs.

🛠️ How HPA Works

Metrics Server collects resource usage
CPU, memory usage, or custom metrics (Prometheus, custom APIs).
HPA Controller monitors usage
Compares current usage with the desired target.
Autoscaling happens

If usage > target → adds more pods.

If usage < target → reduces pods (but not below minimum).

📊 Formula Behind HPA
Desired Replicas = Current Replicas × (Current Metric / Target Metric)


👉 Example:

Current replicas = 2

Target CPU utilization = 50%

Current CPU utilization = 100%

Desired replicas = 2 × (100/50) = 4

So HPA will scale pods from 2 → 4.

📌 Example YAML (CPU-based HPA)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50


➡️ This means:

Maintain between 2 and 10 replicas.
Keep average CPU usage at 50% across pods.

🔑 Key Points

Needs metrics-server installed in the cluster.
Works with CPU/Memory or custom metrics.
Prevents under/over-provisioning of pods.
Common in microservices, APIs, and web apps where load fluctuates.

⚖️ Analogy

Imagine a restaurant 🍴:

Pods = waiters.

CPU utilization = number of customers each waiter is serving.

If too many customers come in (high CPU load), HPA hires more waiters (adds pods).

If the restaurant is empty, it reduces waiters (scales down pods).

✅ In short:
HPA automatically scales pods horizontally (more/less replicas) based on resource usage or custom metrics → keeping apps responsive and efficient.

10. The Kubernetes (K8s) scheduler is a core component of the control plane that is responsible for assigning Pods to nodes in a cluster. Essentially, it decides where each Pod should run based on available resources, policies, and constraints. Here's a breakdown of its role:

1. Pod Placement

The scheduler looks at all unscheduled Pods (those without a node assigned yet).

It selects a suitable node where the Pod can run.

2. Resource Awareness

It considers CPU, memory, GPU, and other resource requests specified in the Pod spec.

Ensures that the node has enough available resources to run the Pod without overloading it.

3. Constraints and Policies

Takes into account node selectors, taints/tolerations, affinities, and anti-affinities.

Respects policies like Pod spreading or custom scheduling rules.

4. Scoring and Prioritization

It evaluates candidate nodes using predicates (filters) and priorities (scores).

Chooses the “best” node based on scores, balancing load and efficiency.

5. Binding Pods to Nodes

Once a node is selected, the scheduler binds the Pod to that node.

After binding, the kubelet on that node takes over to start the container.

Summary:

The Kubernetes scheduler ensures optimal placement of Pods in the cluster based on resources, constraints, and policies, enabling efficient utilization of cluster resources and maintaining workload reliability.

Security and Configurations
----------------------------
10. What are Config maps and secrets?

In Kubernetes, ConfigMaps and Secrets are used to manage configuration data separately from application code, but they differ in sensitivity and purpose. Here's a clear breakdown:

1. ConfigMap

Purpose:

Stores non-sensitive configuration data as key-value pairs.

Helps keep your application code separate from environment-specific configuration.

Use Cases:

Database URLs (non-sensitive)

Application settings (like log levels, feature flags)

Configuration files

Example:

apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  LOG_LEVEL: "DEBUG"
  APP_MODE: "production"


Ways to Use ConfigMap:

As environment variables in a Pod

As command-line arguments

Mounted as files inside a container

2. Secret

Purpose:

Stores sensitive data such as passwords, tokens, keys.

Encodes data in base64 (not fully encrypted by default, but safer than storing plain text).

Use Cases:

Database passwords

API keys or tokens

TLS certificates

Example:

apiVersion: v1
kind: Secret
metadata:
  name: db-secret
type: Opaque
data:
  DB_USERNAME: YWRtaW4=   # base64 for 'admin'
  DB_PASSWORD: cGFzc3dvcmQ=  # base64 for 'password'


Ways to Use Secret:

As environment variables in a Pod

Mounted as files (e.g., certificates)

Used by Kubernetes controllers for authentication

✅ Key Differences
Feature	ConfigMap	Secret
Data type	Non-sensitive	Sensitive
Storage	Plain text	Base64 encoded (optional encryption)
Use cases	App settings, configs	Passwords, tokens, certificates
Security	Not encrypted	Should be treated securely

11. How do you secure a k8s cluster?

Securing a Kubernetes cluster is multi-layered, because a cluster has many components: API server, nodes, pods, networking, etc. Here’s a structured approach to securing a K8s cluster:

1. Secure the Control Plane

API Server Access

Enable RBAC (Role-Based Access Control) to control who can do what.

Use strong authentication (OIDC, client certificates, or cloud IAM).

Enable audit logging to track all API requests.

ETCD Security

Encrypt data at rest using ETCD encryption.

Restrict ETCD access to only the API server.

2. Secure the Nodes

Keep nodes patched and updated.

Run containers as non-root users whenever possible.

Use container runtime security best practices (e.g., Docker Bench for Security).

Limit SSH access and use firewalls.

3. Secure Networking

Enable network policies to restrict pod-to-pod communication.

Use TLS for all communication (API server, etcd, kubelet).

Restrict external exposure; don’t expose kubelet API to the public.

4. Secure Pods and Containers

Use Pod Security Policies / Pod Security Admission (restrict privileged containers, hostPath mounts).

Set resource limits to prevent abuse or DoS attacks.

Avoid storing secrets in environment variables if possible; use Secrets.

5. Secure Supply Chain

Scan container images for vulnerabilities using tools like Trivy or Clair.

Use signed images and enforce imagePullPolicy for trusted images only.

6. Monitoring and Auditing

Enable logging and monitoring (Prometheus, ELK, Grafana) to detect anomalies.

Use Kubernetes audit logs to track who did what.

Implement alerts for suspicious activity.

7. Use Cloud/Managed Security Features (if applicable)

Managed Kubernetes (GKE, EKS, AKS) often has built-in features like:

Node auto-updates

Automatic RBAC and IAM integration

Network isolation and firewall rules

✅ Summary

K8s security is about layered defense:

Control plane → secure access and audit.

Nodes → patching, least privilege.

Network → isolate traffic.

Pods → limit privileges.

Supply chain → trusted images.

Monitoring → detect and respond to threats.

Advanced Topics:

12. Explain the difference between Statefulsets and Deployments?

In Kubernetes, both Deployments and StatefulSets are controllers used to manage Pods, but they serve different use cases, especially around state and identity. Here's a detailed comparison:

1. Deployment

Purpose:

Manages stateless applications (no persistent state needed).

Key Characteristics:

Pods are interchangeable; any Pod can be replaced by another.

Uses ReplicaSets to maintain the desired number of Pods.

Pods are assigned random names (e.g., myapp-5f8d9b7f9c-abcde).

Scaling up or down is easy and fast.

Rolling updates are simple.

Example Use Cases:

Web servers (Nginx, Apache)

Stateless APIs

2. StatefulSet

Purpose:

Manages stateful applications where identity and storage matter.

Key Characteristics:

Pods have stable, unique identities (persistent hostname and network ID).

Each Pod can have its own persistent volume that sticks even if the Pod is deleted.

Pods are created in a defined order (myapp-0, myapp-1, myapp-2).

Updates can be ordered and controlled, which is important for some stateful workloads.

Example Use Cases:

Databases (MySQL, PostgreSQL)

Kafka, ZooKeeper, Redis

Applications that require persistent storage and stable identity

✅ Key Differences
Feature	Deployment	StatefulSet
Use Case	Stateless apps	Stateful apps
Pod Identity	Interchangeable, random names	Stable, unique names
Storage	Ephemeral (optional PVC)	Persistent, each Pod gets its own PVC
Scaling	Simple, unordered,	Ordered, Pod-by-Pod
Rolling Updates, Parallel or batch	Ordered, controlled
Examples	Nginx, API servers	MySQL, Kafka, Redis

In short:

Use Deployments when your app is stateless and any Pod can serve any request.

Use StatefulSets when your app needs stable identity and persistent storage.

13. How do u perform a rolling updates vs a blue-green deployment in k8s?

In Kubernetes, both Rolling Updates and Blue-Green Deployments are strategies for updating applications, but they work differently. Here’s a detailed breakdown:

1. Rolling Update

Definition:

Updates the application gradually without downtime.

Kubernetes replaces old Pods with new ones incrementally, respecting the maxUnavailable and maxSurge settings.

How it works:

Specify a new version of the container image in the Deployment.

Kubernetes creates new Pods with the updated version.

Old Pods are terminated gradually as new Pods become ready.

Traffic is automatically routed to new Pods.

Command Example:

kubectl set image deployment/myapp myapp-container=myapp:v2
kubectl rollout status deployment/myapp


Pros:

No downtime if configured correctly.

Simple and automated with Deployment.

Cons:

Rollback may be slower if issues occur.

All Pods share the same environment, so bugs could affect users immediately.

2. Blue-Green Deployment

Definition:

Deploys a completely new environment (green) alongside the current one (blue).

Switches traffic to the new version only when it’s ready.

How it works:

Deploy a new version of the app as a separate Deployment or namespace (myapp-green).

Test the new environment without affecting the live version (myapp-blue).

Once verified, switch the Service to point to the green Pods.

Optionally, delete or keep the blue environment as a rollback.

Command Example:

# Create green deployment
kubectl apply -f myapp-green.yaml

# Switch service selector
kubectl patch svc myapp -p '{"spec":{"selector":{"app":"myapp-green"}}}'


Pros:

Zero downtime and safe testing before traffic switch.

Easy rollback by switching Service back to blue.

Cons:

Requires double the resources temporarily (both blue and green running).

More complex setup.

✅ Key Differences
Feature	Rolling Update	Blue-Green Deployment
Downtime	Minimal, depends on readiness	Zero downtime if done correctly
Resource Usage	Normal (replaces pods gradually)	Double resources temporarily
Rollback	Automatic via Deployment history	Switch Service back to old version
Complexity	Simple (native Deployment)	Higher (two environments)
Testing New Version	Limited (live traffic)	Full testing without affecting live users

Summary:

Rolling Update: Replace old Pods gradually; simpler but all traffic hits new Pods immediately.

Blue-Green: Stand up a new environment and switch traffic; safer but more resource-intensive.

14. How do you debug a pod stuck in CrashloopBackoff?
    When a Pod is stuck in CrashLoopBackOff, it means the container keeps crashing repeatedly. Debugging involves identifying why the container is failing and fixing it. Here’s a step-by-step guide:

1. Check Pod Status
kubectl get pods


Look for the RESTARTS column — high numbers indicate repeated crashes.

2. Describe the Pod
kubectl describe pod <pod-name>


Check for:

Events at the bottom (e.g., OOMKilled, image pull errors)

Node scheduling issues

Volume mount problems

3. Check Container Logs
kubectl logs <pod-name>


If the Pod has multiple containers, specify the container:

kubectl logs <pod-name> -c <container-name>


Logs often reveal runtime errors or exceptions causing the crash.

4. Get Previous Logs (Optional)

If the container keeps restarting:

kubectl logs <pod-name> --previous


This shows logs from the previous container instance, often critical for debugging transient crashes.

5. Inspect Liveness/Readiness Probes

Misconfigured liveness probes can trigger restarts.

Check the Pod spec for livenessProbe and readinessProbe values.

Try disabling them temporarily to see if the Pod stabilizes.

6. Check Resource Limits

If the Pod is OOMKilled (out of memory), increase resources.limits.memory.

Too low CPU limits can also cause throttling and crashes.

7. Run Pod Interactively
kubectl run -it --rm debug-pod --image=<same-image> -- /bin/bash


Start a temporary Pod with the same image and debug interactively.

Check configuration files, environment variables, or start commands.

8. Check ConfigMaps, Secrets, and Environment Variables

Ensure all dependencies are mounted correctly.

Wrong environment variables or missing secrets can crash the container immediately.

9. Review Image and Entrypoint

Verify the container image exists and entrypoint command is correct.

Sometimes an invalid CMD or ENTRYPOINT causes instant crashes.

✅ Summary Steps

kubectl describe pod <pod> → check events.

kubectl logs <pod> / --previous → check errors.

Check probes, resources, env variables, volumes.

Run a temporary interactive pod for testing.

Fix the root cause (image, command, configuration, or resource limits).

15. What is the role of etcd in K8S?

In Kubernetes, etcd is a key component of the control plane and acts as the cluster’s primary data store. Its main role is to store and manage all cluster state. Here's a detailed explanation:

1. What etcd Stores

etcd is a distributed, consistent key-value store, and it stores:

Cluster configuration (nodes, namespaces, labels)

Pod specs (Deployments, StatefulSets, ReplicaSets)

Service definitions and endpoints

Secrets and ConfigMaps

Status of all resources

Essentially, every object in Kubernetes is persisted in etcd.

2. Key Roles of etcd

Source of Truth

etcd holds the authoritative state of the cluster.

The API server queries etcd whenever it needs the current cluster state.

Consistency and Reliability

etcd uses the Raft consensus algorithm, ensuring data consistency across multiple nodes.

Even if some control plane nodes fail, the cluster state remains consistent.

Supports Cluster Recovery

If the API server or nodes crash, etcd allows restoring the cluster to its previous state.

Facilitates Watchers

Controllers and operators can watch etcd for changes, enabling Kubernetes to respond automatically (e.g., creating new Pods, scaling).

3. How it Fits in the Control Plane
User/CLI/Controller → API Server → etcd (persistent store)


API Server: talks to etcd to read/write state.

Controllers/Schedulers: watch API server (which reads from etcd) to act on changes.

Nodes/Kubelets: get desired state from the API server, which reflects etcd.

✅ Key Points
Feature	Description
Type	Distributed key-value store
Purpose	Stores cluster state, configuration, secrets
Consistency	Strong consistency via Raft
Critical for	Cluster recovery, scaling, scheduling, updates

In short:

etcd is the single source of truth for a Kubernetes cluster. Every object, configuration, and secret lives in etcd, and the API server relies on it to ensure the cluster functions correctly.


16. How does RBAC work in K8S?

In Kubernetes, RBAC (Role-Based Access Control) is the mechanism to control who can do what in the cluster. It’s a key part of securing your Kubernetes environment. Here’s a detailed explanation:

1. Key Concepts

Role / ClusterRole

Defines permissions (verbs like get, list, create, delete) on resources (pods, deployments, etc.).

Role: permissions within a specific namespace.

ClusterRole: permissions cluster-wide.

RoleBinding / ClusterRoleBinding

Associates a Role or ClusterRole with users, groups, or service accounts.

RoleBinding: applies within a namespace.

ClusterRoleBinding: applies cluster-wide.

Subjects

The users, groups, or service accounts that the Role is granted to.

2. How RBAC Works

Request Flow:

User → API Server → RBAC authorization → Allowed or Denied


When a user or service account makes a request, the API server checks RBAC rules.

If a Role/ClusterRole grants permission for the requested verb/resource, the action is allowed. Otherwise, it is denied.

Evaluation:

RBAC rules are additive, meaning multiple roles can combine to give cumulative permissions.

Kubernetes checks all RoleBindings/ClusterRoleBindings relevant to the user.

3. Example: Giving a Service Account Access to Pods in a Namespace

Step 1: Create a Role

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]


Step 2: Bind the Role to a User or ServiceAccount

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: dev
subjects:
- kind: ServiceAccount
  name: my-service-account
  namespace: dev
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io


Now, my-service-account can get, list, and watch Pods in the dev namespace but cannot create or delete them.

4. Best Practices

Least Privilege: Grant only the permissions that are necessary.

Use ServiceAccounts for applications rather than giving user accounts access.

Prefer ClusterRoles sparingly; namespace-scoped Roles are safer.

Audit RBAC rules regularly to prevent privilege creep.

✅ Summary

RBAC in Kubernetes allows fine-grained access control:

Roles define what can be done.

RoleBindings define who can do it.

API server checks the bindings every time a request is made to enforce security.

 



